{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"JElpI7hB6G9g"},"outputs":[],"source":["import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from torch.nn.utils.rnn import pad_sequence\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vbLSK7-Q6Sze"},"outputs":[],"source":["df = pd.read_csv('/content/final_tweets.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cpYrQFe86alt"},"outputs":[],"source":["sentiment_mapping = {1: 2, -1: 0, 0: 1}  # Map old sentiment values to new labels\n","df['polarity'] = df['sentiment'].map(sentiment_mapping)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A08XcX406dTO"},"outputs":[],"source":["import nltk\n","\n","# Download 'punkt_tab' to enable sentence tokenization in word_tokenize.\n","nltk.download('punkt_tab')\n","\n","# Download 'punkt' to enable word tokenization.\n","nltk.download('punkt')\n","\n","# Tokenize each tweet\n","from nltk.tokenize import word_tokenize  # Explicitly import word_tokenize\n","\n","df['tokens'] = df['Tweets'].apply(word_tokenize)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YPOnpVjx6jW8"},"outputs":[],"source":["from gensim.models import Word2Vec\n","\n","# Prepare data for Word2Vec training\n","sentences = df['tokens'].tolist()  # List of tokenized sentences\n","word2vec = Word2Vec(sentences, vector_size=1000, window=5, min_count=1, workers=4)\n","embedding_dim = word2vec.vector_size\n","\n","# Convert each tokenized sentence into a list of word embeddings\n","def text_to_embedding(tokens):\n","    return [word2vec.wv[word] if word in word2vec.wv else [0]*embedding_dim for word in tokens]\n","\n","df['embedded_text'] = df['tokens'].apply(text_to_embedding)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gva_kcx260yS"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","import torch\n","from torch.utils.data import Dataset, DataLoader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jSaYKQhi63x5"},"outputs":[],"source":["# Split data and labels\n","train_data, test_data, train_labels, test_labels = train_test_split(\n","    df['embedded_text'].tolist(),  # List of embedded texts\n","    df['polarity'].tolist(),       # List of polarity labels\n","    test_size=0.2,                 # 20% for testing, 80% for training\n","    random_state=42                # For reproducibility\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bUk9H9lq662a"},"outputs":[],"source":["import torch\n","from torch.nn.utils.rnn import pad_sequence\n","\n","class TweetsDataset(torch.utils.data.Dataset):\n","    def __init__(self, data, labels):\n","        # Filter out data points with NaN labels during initialization\n","        self.data = [d for d, l in zip(data, labels) if not torch.isnan(torch.tensor(l)).item()]\n","        self.labels = [l for l in labels if not torch.isnan(torch.tensor(l)).item()]\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        data = torch.tensor(self.data[idx], dtype=torch.float32)\n","        label = torch.tensor(int(self.labels[idx]), dtype=torch.long)  # No need to check for NaN anymore\n","        return data, label"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mckD6izz6-KA"},"outputs":[],"source":["def collate_fn(batch):\n","    # Separate features (x) and labels (y)\n","    features = [item[0] for item in batch]\n","    labels = [item[1] for item in batch]\n","\n","    # Pad the features using pad_sequence\n","    features_padded = pad_sequence(features, batch_first=True, padding_value=0)  # Assumes 0 is the padding token\n","\n","    # Stack the labels to create a tensor\n","    labels = torch.tensor(labels)\n","\n","    return features_padded, labels\n","\n","train_dataset = TweetsDataset(train_data, train_labels) # This line was missing\n","test_dataset = TweetsDataset(test_data, test_labels)   # This line was missing\n","\n","# Create DataLoaders using the custom collate function\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fAJYKzOZ7BVI"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","class LSTMModel(nn.Module):\n","    def __init__(self, embedding_dim, hidden_dim, output_dim):\n","        super(LSTMModel, self).__init__()\n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n","        self.dropout = nn.Dropout(0.2)  # Add dropout with a rate of 0.2\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x):\n","        _, (hidden, _) = self.lstm(x)\n","        out = self.dropout(hidden[-1])  # Apply dropout to the hidden state\n","        out = self.fc(out)\n","        return out\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2vD6F-Py7Dr2"},"outputs":[],"source":["# Hyperparameters\n","embedding_dim = 1000  # Should match the Word2Vec embedding size\n","hidden_dim = 32\n","output_dim = 3  # For three classes: negative, neutral, positive\n","learning_rate = 0.001\n","num_epochs = 10\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wp3aiChM7Ie6"},"outputs":[],"source":["# Initialize model, loss function, and optimizer\n","lstm_model = LSTMModel(embedding_dim, hidden_dim, output_dim)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(lstm_model.parameters(), lr=learning_rate)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EvcukyGt7MZQ","outputId":"cfc0e828-1fbf-45ec-c26c-968f7938fbde"},"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["<ipython-input-8-02caa065c00e>:14: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n","  data = torch.tensor(self.data[idx], dtype=torch.float32)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/10], LSTM Loss: 0.9225, Accuracy: 55.87%\n","Epoch [2/10], LSTM Loss: 0.8149, Accuracy: 60.94%\n","Epoch [3/10], LSTM Loss: 0.7576, Accuracy: 64.87%\n","Epoch [4/10], LSTM Loss: 0.6983, Accuracy: 68.88%\n","Epoch [5/10], LSTM Loss: 0.6678, Accuracy: 70.85%\n","Epoch [6/10], LSTM Loss: 0.6437, Accuracy: 72.38%\n","Epoch [7/10], LSTM Loss: 0.6292, Accuracy: 73.05%\n","Epoch [8/10], LSTM Loss: 0.6136, Accuracy: 73.82%\n","Epoch [9/10], LSTM Loss: 0.6030, Accuracy: 74.43%\n","Epoch [10/10], LSTM Loss: 0.5918, Accuracy: 74.97%\n"]}],"source":["import matplotlib.pyplot as plt\n","\n","epoch_accuracy = []  # Initialize an empty list to store epoch accuracies\n","\n","\n","def train_model(model, optimizer, loader):\n","    model.train()  # Set the model to training mode\n","    total_loss = 0\n","    correct_predictions = 0\n","    total_samples = 0\n","\n","    for x, y in loader:\n","        optimizer.zero_grad()  # Clear gradients\n","        output = model(x)  # Forward pass\n","        loss = criterion(output, y)  # Compute loss\n","        loss.backward()  # Backward pass\n","        optimizer.step()  # Update weights\n","        total_loss += loss.item()\n","\n","        # Calculate accuracy\n","        _, predicted = torch.max(output, 1)  # Get predicted class labels\n","        total_samples += y.size(0)\n","        correct_predictions += (predicted == y).sum().item()\n","\n","    accuracy = correct_predictions / total_samples\n","    return total_loss / len(loader), accuracy\n","\n","\n","# Training the model\n","for epoch in range(num_epochs):\n","    lstm_loss, accuracy = train_model(lstm_model, optimizer, train_loader)\n","    epoch_accuracy.append(accuracy)\n","    print(f'Epoch [{epoch+1}/{num_epochs}], LSTM Loss: {lstm_loss:.4f}, Accuracy: {accuracy * 100:.2f}%')\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kmHE52AjWMLH","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4992f1d6-aa15-479d-c1b2-c749367730c4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Test Accuracy: 73.08%\n","Precision: 0.7355\n","Recall: 0.7308\n","F1-Score: 0.7189\n"]}],"source":["from sklearn.metrics import precision_score, recall_score, f1_score\n","\n","def evaluate_model_with_metrics(model, loader):\n","    model.eval()  # Set the model to evaluation mode\n","    all_targets = []\n","    all_predictions = []\n","\n","    with torch.no_grad():  # No need to calculate gradients\n","        for x, y in loader:\n","            output = model(x)  # Forward pass\n","            _, predicted = torch.max(output, 1)  # Get class with max probability\n","\n","            # Collect predictions and actual labels\n","            all_predictions.extend(predicted.cpu().numpy())\n","            all_targets.extend(y.cpu().numpy())\n","\n","    # Calculate Accuracy\n","    accuracy = (np.array(all_predictions) == np.array(all_targets)).sum() / len(all_targets)\n","\n","    # Calculate Precision, Recall, F1-Score\n","    precision = precision_score(all_targets, all_predictions, average='weighted')\n","    recall = recall_score(all_targets, all_predictions, average='weighted')\n","    f1 = f1_score(all_targets, all_predictions, average='weighted')\n","\n","    return accuracy, precision, recall, f1\n","\n","# Evaluate the LSTM Model\n","accuracy, precision, recall, f1 = evaluate_model_with_metrics(lstm_model, test_loader)\n","\n","# Print the metrics\n","print(f'Test Accuracy: {accuracy*100:.2f}%')\n","print(f'Precision: {precision:.4f}')\n","print(f'Recall: {recall:.4f}')\n","print(f'F1-Score: {f1:.4f}')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RM9_IgpXG6dU"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wRGyH9t1bOsl"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","class RNNModel(nn.Module):\n","    def __init__(self, embedding_dim, hidden_dim, output_dim):\n","        super(RNNModel, self).__init__()\n","        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x):\n","        _, hidden = self.rnn(x)  # Use the last hidden state output\n","        out = self.fc(hidden[-1])\n","        return out\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8p9w6sIqiq4e"},"outputs":[],"source":["# Hyperparameters\n","embedding_dim = 1000  # Should match the Word2Vec embedding size\n","hidden_dim = 32\n","output_dim = 3  # For three classes: negative, neutral, positive\n","learning_rate = 0.001\n","num_epochs = 10\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ttnk07VRitzp"},"outputs":[],"source":["# Initialize the RNN model, loss function, and optimizer\n","rnn_model = RNNModel(embedding_dim, hidden_dim, output_dim)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(rnn_model.parameters(), lr=learning_rate)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e38tze-oiv8g","colab":{"base_uri":"https://localhost:8080/"},"outputId":"df2b3281-e0f8-4ece-9387-6bef2c9d4a2e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/10], RNN Loss: 1.0056, Training Accuracy: 50.43%\n","Epoch [2/10], RNN Loss: 0.9896, Training Accuracy: 52.13%\n","Epoch [3/10], RNN Loss: 0.9890, Training Accuracy: 51.97%\n","Epoch [4/10], RNN Loss: 0.9877, Training Accuracy: 52.04%\n","Epoch [5/10], RNN Loss: 0.9854, Training Accuracy: 52.53%\n","Epoch [6/10], RNN Loss: 0.9850, Training Accuracy: 52.50%\n","Epoch [7/10], RNN Loss: 0.9840, Training Accuracy: 52.57%\n","Epoch [8/10], RNN Loss: 0.9815, Training Accuracy: 52.94%\n","Epoch [9/10], RNN Loss: 0.9812, Training Accuracy: 52.79%\n","Epoch [10/10], RNN Loss: 0.9799, Training Accuracy: 52.98%\n"]}],"source":["import matplotlib.pyplot as plt\n","\n","# Function to train the model and calculate accuracy\n","def train_model(model, optimizer, loader):\n","    model.train()  # Set the model to training mode\n","    total_loss = 0\n","    correct_predictions = 0\n","    total_samples = 0\n","\n","    for x, y in loader:\n","        optimizer.zero_grad()  # Clear gradients\n","        output = model(x)  # Forward pass\n","        loss = criterion(output, y)  # Compute loss\n","        loss.backward()  # Backward pass\n","        optimizer.step()  # Update weights\n","        total_loss += loss.item()\n","\n","        # Calculate accuracy\n","        _, predicted = torch.max(output, 1)  # Get predicted class labels\n","        total_samples += y.size(0)\n","        correct_predictions += (predicted == y).sum().item()\n","\n","    # Calculate average loss and accuracy\n","    average_loss = total_loss / len(loader)\n","    accuracy = correct_predictions / total_samples  # Accuracy in decimal form\n","    return average_loss, accuracy\n","\n","# Training the RNN model\n","train_losses = []\n","train_accuracies = []\n","\n","for epoch in range(num_epochs):\n","    # Train and record the loss and accuracy\n","    rnn_loss, train_accuracy = train_model(rnn_model, optimizer, train_loader)\n","    train_losses.append(rnn_loss)\n","    train_accuracies.append(train_accuracy)\n","\n","    print(f'Epoch [{epoch+1}/{num_epochs}], RNN Loss: {rnn_loss:.4f}, Training Accuracy: {train_accuracy * 100:.2f}%')\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bGbBe6rzi012","colab":{"base_uri":"https://localhost:8080/"},"outputId":"bbcb9167-1255-4c43-907b-c249e247d1c8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Final Test Accuracy of RNN Model: 50.7000%\n","Precision: 0.4051\n","Recall: 0.5070\n","F1-Score: 0.3818\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"]}],"source":["from sklearn.metrics import precision_score, recall_score, f1_score\n","\n","def evaluate_model_with_metrics(model, loader):\n","    model.eval()  # Set the model to evaluation mode\n","    all_targets = []\n","    all_predictions = []\n","\n","    with torch.no_grad():  # No need to calculate gradients\n","        for x, y in loader:\n","            output = model(x)  # Forward pass\n","            _, predicted = torch.max(output, 1)  # Get class with max probability\n","\n","            # Append predictions and actual labels\n","            all_predictions.extend(predicted.cpu().numpy())\n","            all_targets.extend(y.cpu().numpy())\n","\n","    # Accuracy\n","    accuracy = (np.array(all_predictions) == np.array(all_targets)).sum() / len(all_targets)\n","\n","    # Precision, Recall, F1-Score\n","    precision = precision_score(all_targets, all_predictions, average='weighted')\n","    recall = recall_score(all_targets, all_predictions, average='weighted')\n","    f1 = f1_score(all_targets, all_predictions, average='weighted')\n","\n","    return accuracy, precision, recall, f1\n","\n","# Final Test Evaluation with Metrics\n","accuracy, precision, recall, f1 = evaluate_model_with_metrics(rnn_model, test_loader)\n","\n","# Print Metrics\n","print(f\"Final Test Accuracy of RNN Model: {accuracy*100:.4f}%\")\n","print(f\"Precision: {precision*100:.4f}\")\n","print(f\"Recall: {recall*100:.4f}\")\n","print(f\"F1-Score: {f1*100:.4f}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4pwZBbt1uhZK"},"outputs":[],"source":["class LSTMModel(nn.Module):\n","    def __init__(self, embedding_dim, hidden_dim, output_dim):\n","        super(LSTMModel, self).__init__()\n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x):\n","        _, (hidden, _) = self.lstm(x)\n","        out = self.fc(hidden[-1])\n","        return out  # No softmax, we’ll use raw scores\n","\n","\n","class RNNModel(nn.Module):\n","    def __init__(self, embedding_dim, hidden_dim, output_dim):\n","        super(RNNModel, self).__init__()\n","        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x):\n","        _, hidden = self.rnn(x)\n","        out = self.fc(hidden[-1])\n","        return out  # No softmax, we’ll use raw scores\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e2Wl0x48upU8"},"outputs":[],"source":["def ensemble_predict_majority_voting(lstm_model, rnn_model, loader):\n","    lstm_model.eval()\n","    rnn_model.eval()\n","    correct, total = 0, 0\n","    with torch.no_grad():\n","        for x, y in loader:\n","            # Get class predictions from both models\n","            lstm_output = lstm_model(x)  # LSTM model output\n","            _, lstm_pred = torch.max(lstm_output, 1)  # LSTM model prediction\n","\n","            rnn_output = rnn_model(x)  # RNN model output\n","            _, rnn_pred = torch.max(rnn_output, 1)  # RNN model prediction\n","\n","            # Combine predictions using majority voting\n","            final_pred = []\n","            for i in range(len(lstm_pred)):\n","                # If both models agree, use that prediction\n","                if lstm_pred[i] == rnn_pred[i]:\n","                    final_pred.append(lstm_pred[i])\n","                else:\n","                    # In case of a tie, use LSTM's prediction as the default\n","                    final_pred.append(lstm_pred[i])\n","\n","            final_pred = torch.stack(final_pred)\n","\n","            # Count correct predictions\n","            correct += (final_pred == y).sum().item()\n","            total += y.size(0)\n","\n","    return correct / total\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6EGedj6Uusrs","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1721b83e-3932-40e4-9930-cfdd593cd9c0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Ensemble Model Test Accuracy (Majority Voting): 0.7308\n"]}],"source":["# Evaluate ensemble model on test set with majority voting\n","ensemble_accuracy = ensemble_predict_majority_voting(lstm_model, rnn_model, test_loader)\n","print(f'Ensemble Model Test Accuracy (Majority Voting): {ensemble_accuracy:.4f}')\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}